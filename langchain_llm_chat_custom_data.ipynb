{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vF5U5KuxigVirzmdYxyRYo_9lN6dGCJM","timestamp":1699612988412}],"authorship_tag":"ABX9TyNnzkHTT9VX6PN1Dqla14xZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## LangChain\n"],"metadata":{"id":"28JFyim9heen"}},{"cell_type":"code","source":["pip install --upgrade openai\n"],"metadata":{"id":"Aop0EeGSZm9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pip show openai"],"metadata":{"id":"OkI7Zfm1BMwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install langchain"],"metadata":{"id":"AipAYhUQ1auE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install \"langserve[all]\""],"metadata":{"id":"kKdM7I91EJ0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ipqUaWDOYtRc"},"outputs":[],"source":["import os\n","import sys\n","import openai"]},{"cell_type":"code","source":["openai.api_key = 'sk-xxxxxxx' # enter your key\n","OPENAI_API_KEY = openai.api_key\n","#os.environ['OPENAI_API_KEY'] = openai.api_key\n"],"metadata":{"id":"_b-29x-_ZyKn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LLM / Chat Model\n","There are two types of language models:\n","\n","LLM: underlying model takes a string as input and returns a string\n","\n","ChatModel: underlying model takes a list of messages as input and returns a message\n","\n","LLM.invoke: Takes in a string, returns a string.\n","ChatModel.invoke: Takes in a list of BaseMessage, returns a BaseMessage"],"metadata":{"id":"yUzJ60DPf9Ld"}},{"cell_type":"code","source":["from langchain.llms import OpenAI\n","llm = OpenAI()\n","\n","from langchain.chat_models import ChatOpenAI\n","chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"],"metadata":{"id":"7yLdBuXnCFvW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LLM and ChatModel objects are effectively configuration objects. You can initialize them with parameters like temperature and others, and pass them around."],"metadata":{"id":"SefLmVh9gOhs"}},{"cell_type":"code","source":["from langchain.schema import HumanMessage\n","\n","text = \"What would be a good company name for a company that makes colorful socks?\"\n","messages = [HumanMessage(content=text)]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDRqZqAdfxFY","executionInfo":{"status":"ok","timestamp":1699657910973,"user_tz":0,"elapsed":4518,"user":{"displayName":"Hamid Hassan","userId":"07854950186804749165"}},"outputId":"f1e682cc-0ec2-4413-c5fd-1dcfa628f364"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='ColorfulStride')"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["llm.invoke(text)\n","# >> Feetful of Fun"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"PBkxHfmOgh4e","executionInfo":{"status":"ok","timestamp":1699657969682,"user_tz":0,"elapsed":654,"user":{"displayName":"Hamid Hassan","userId":"07854950186804749165"}},"outputId":"f92c7d2f-1015-4602-b34d-dcbfe4061de4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nBright Socks Co.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["chat_model.invoke(messages)\n","# >> AIMessage(content=\"Socks O'Color\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Da95BzfUgjIT","executionInfo":{"status":"ok","timestamp":1699658026333,"user_tz":0,"elapsed":1132,"user":{"displayName":"Hamid Hassan","userId":"07854950186804749165"}},"outputId":"69ba4663-9df2-4c75-be1d-8b49568d76a8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='SockSplash')"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## Prompt templates\n","Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\n","\n","PromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple - for example, a prompt to produce the above string would just be:\n"],"metadata":{"id":"uDnAajLrhF6r"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n","prompt.format(product=\"colorful socks\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"l2MGknrJhcIj","executionInfo":{"status":"ok","timestamp":1699658313314,"user_tz":0,"elapsed":207,"user":{"displayName":"Hamid Hassan","userId":"07854950186804749165"}},"outputId":"83cdc467-eb4d-4c67-dbac-ec906080aaa3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'What is a good name for a company that makes colorful socks?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["PromptTemplates can also be used to produce a list of messages.\n","\n","In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content. Let's take a look at this below:"],"metadata":{"id":"tfp_hfSliLnn"}},{"cell_type":"code","source":["from langchain.prompts.chat import ChatPromptTemplate\n","\n","template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n","human_template = \"{text}\"\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", template),\n","    (\"human\", human_template),\n","])\n","\n","chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQc0vCZ-iUga","executionInfo":{"status":"ok","timestamp":1699658457788,"user_tz":0,"elapsed":5,"user":{"displayName":"Hamid Hassan","userId":"07854950186804749165"}},"outputId":"7f7498b9-3425-4bfe-c7cb-b21cf74f7129"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[SystemMessage(content='You are a helpful assistant that translates English to French.'),\n"," HumanMessage(content='I love programming.')]"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## Output parsers\n","\n","OutputParsers convert the raw output of a language model into a format that can be used downstream. There are few main types of OutputParsers, including:\n","\n","-> Convert text from LLM into structured information (e.g. JSON)\n","\n","-> Convert a ChatMessage into just a string\n","\n","-> Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n","\n","Below we will write our own output parser - one that converts a comma separated list into a list."],"metadata":{"id":"PhU9C0mZi9Lp"}},{"cell_type":"code","source":["from langchain.schema import BaseOutputParser\n","\n","class CommaSeparatedListOutputParser(BaseOutputParser):\n","    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n","\n","\n","    def parse(self, text: str):\n","        \"\"\"Parse the output of an LLM call.\"\"\"\n","        return text.strip().split(\", \")\n","\n","CommaSeparatedListOutputParser().parse(\"hi, bye\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQdZcKYTjNWW","executionInfo":{"status":"ok","timestamp":1699658681229,"user_tz":0,"elapsed":14,"user":{"displayName":"Hamid Hassan","userId":"07854950186804749165"}},"outputId":"bb23136e-c191-4b91-c770-6d6ed6f9e617"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hi', 'bye']"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## Composing with LCEL\n","We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let's see it in action!"],"metadata":{"id":"M0oDiCGmjhdT"}},{"cell_type":"code","source":["from typing import List\n","\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema import BaseOutputParser\n","\n","class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n","    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n","\n","\n","    def parse(self, text: str) -> List[str]:\n","        \"\"\"Parse the output of an LLM call.\"\"\"\n","        return text.strip().split(\", \")\n","\n","template = \"\"\"You are a helpful assistant who generates comma separated lists.\n","A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n","ONLY return a comma separated list, and nothing more.\"\"\"\n","human_template = \"{text}\"\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", template),\n","    (\"human\", human_template),\n","])\n","chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n","chain.invoke({\"text\": \"colors\"})\n","# >> ['red', 'blue', 'green', 'yellow', 'orange']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjtzquSVjlB_","executionInfo":{"status":"ok","timestamp":1699658764265,"user_tz":0,"elapsed":9692,"user":{"displayName":"Hamid Hassan","userId":"07854950186804749165"}},"outputId":"b7f5fcc9-154a-4d28-b553-07662f72a7b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['red', 'blue', 'green', 'yellow', 'orange']"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# from langchain.document_loaders import TextLoader\n","# from langchain.indexes import VectorstoreIndexCreator"],"metadata":{"id":"xtE34Yrk_CWl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VTmNv8KGC9PY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install chromadb"],"metadata":{"id":"opgQ5jJ8Bis-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install tiktoken"],"metadata":{"id":"rFbfji74B-KU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = TextLoader('Data.txt')"],"metadata":{"id":"-XO1Z4zw_NcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = VectorstoreIndexCreator().from_loaders([loader])"],"metadata":{"id":"yvBDs8T2_XyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"what is Suella Braverman claim\"\n","print(index.query(query, llm=ChatOpenAI()))"],"metadata":{"id":"uDWJwtLkCI8k"},"execution_count":null,"outputs":[]}]}